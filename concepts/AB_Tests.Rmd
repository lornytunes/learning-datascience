---
title: "A/B Tests"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
options(digits=4)
```


```{r simulated_test_data}
set.seed(123515)
ab_data <- tibble(
    group = factor(c(rep('A', 100000), rep('B', 10000))),
    converted = c(rbinom(100000, size = 1, p = 0.05), rbinom(10000, size = 1, p = 0.055))
)
```

```{r response_variable}
ab_data %>% 
    count(group)
```



```{r ab_contingency_table}
(tab <- xtabs(~group + converted, data = ab_data))
```

The contingency table is what statisticians call a sufficient statistic: it contains all we need to know about the experiment outcome. 

We can print the observed conversion rates of the A and B groups using the row proportions.

```{r ab_conversion_rates}
# conversion rates are in column 1
prop.table(tab, 1)
```

```{r ab_common_rate}
# common rate - the overall proportion of conversions
(common_rate <- sum(tab[, '1']) / sum(tab))
```


We see that the A group was measured at near 5%, and the B group was measured at near 6%. What we want to know is this: can we trust this difference? Could such a difference be likely for this sample size due to mere chance and measurement noise? We need to calculate a significance to see if we ran a large enough experiment (obviously, we’d want to design an experiment that was large enough—what we call test power).

What follows are a few good tests that are quick to run.

```{r chisq_test_stats}
addmargins(tab)
# converted totals are the column sums
(nconverted <- margin.table(tab, margin = 2))
# group totals are the row sums
(ngroup <- margin.table(tab, margin = 1))
# Expected = RxC / G
(expected <- (ngroup %*% t(nconverted)) / sum(tab))
```

```{r chisq_test_compute}
# differences between observed and expected
(cvals <- (tab - expected)^2/expected)
# sum them
(cstat <- sum(cvals))
```

```{r chisq_test_evaluation}
# very low p-value
round(1 - pchisq(cstat, 1), 4)
# note the complimentary signs
sign(tab - expected)
# run the test in r
res <- chisq.test(tab, correct = FALSE)
broom::glance(res)
```

```{r chisq_test_cleanup}
rm(nconverted, ngroup, expected, cvals, cstat, res)
```


```{r conversion_counts}
tab
```

```{r compute_odds_ratio}
# conversion odds - the odds of conversion given the group
(odds <- tab[,1]/tab[,2])
# and the odds ratio is
(odds[1] / odds[2])
```


```{r ab_fisher_test}
fisher.test(tab)
```

This is a great result. The p-value (which in this case is the probability of observing a difference this large if we in fact had A=B) is `2.469e-05`, which is very small. This is considered a significant result.

The other thing to look for is the odds ratio: the practical importance of the claimed effect (sometimes also called clinical significance, which is not a statistical significance).

An odds ratio of 1.2 says that we’re measuring a 20% relative improvement in conversion rate between the A and B groups. Whether you consider this large or small (typically, 20% is considered large) is an important business question.



```{r frequentist_significance}
pbinom(
    # we want the probability of being greater than a given q
    lower.tail = FALSE,
    # the probability of seeing at least as many conversions as our observed B group did
    # 1 is subtracted to make the comparsion greater than or equal to
    q = tab['B', '1'] - 1,
    # the total number of trials
    size = sum(tab['B',]),
    # the conversion probability at the estimated common rate
    prob = common_rate
)
```


## Power of tests


To have reliable A/B test results, you must first design and run good A/B tests. We need to defend against two types of errors: failing to see a difference, assuming there is one (described as test power); and seeing a difference, assuming there is not one (described as significance). 

The closer the difference in A and B rates we are trying to measure, the harder it is to have a good probability of getting a correct measurement.

Our only tools are to design experiments where we hope A and B are far apart, or to increase experiment size. A power calculator lets us choose experiment size.